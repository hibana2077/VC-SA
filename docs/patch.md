å¥½çš„ï¼Œæ”¶åˆ°ä½ çš„é™åˆ¶ï¼ˆä¸ç”¨ SSMï¼tensor åˆ†è§£ï¼GNNï¼graphï¼ä½ç§©æ ¸ï¼ToMe/VTMï¼‰ã€‚ä¸‹é¢çµ¦ä½ å…©å€‹**å¯ç›´æ¥æ›¿æ›**çš„ Drop-in æ¨¡çµ„ï¼šä¸€å€‹å–ä»£ã€Œå¯å­¸å¼ frame/token selectionã€ï¼Œä¸€å€‹ï¼ˆå…¶å¯¦çµ¦ä½ å…©ç¨®å¯é¸ï¼‰å–ä»£ã€ŒMem bankã€ï¼Œ**å®Œå…¨éµå®ˆä½ ç¾æœ‰ I/O**ï¼š`selector: xâˆˆ[B,T,N,D] â†’ zâˆˆ[B,Kf,Kt,D]`ã€`membank: zâˆˆ[B,T,M,D] â†’ hâˆˆ[B,T,M,D]`ï¼Œèˆ‡ä½ æª”æ¡ˆè£¡çš„ä»‹é¢/å½¢ç‹€ä¸€è‡´ã€‚  

---

# A) å–ä»£é¸å–å™¨ï¼šFPS + è®ŠåŒ–é»ï¼ˆä¸é ä½ç§©æ ¸ï¼åœ–ï¼‰

**æƒ³æ³•**ï¼š

1. å…ˆä»¥**ç·šä¸Šè®ŠåŒ–é»**ç›´è¦ºæ‰¾ã€Œé—œéµæ™‚åˆ»ã€ï¼ˆç”¨ç°¡å–®çš„ EMA é©…å‹•çš„**æ–°å¥‡åº¦**åˆ†æ•¸ä½œç‚º seedï¼‰ï¼›
2. å†ç”¨**Farthest-Point Samplingï¼ˆFPSï¼‰/k-center çš„ 2-approx**å»æŒ‘å‡º Kf ä»½é‡æœ€ã€Œåˆ†æ•£ã€ä»£è¡¨æ€§é«˜ã€çš„å½±æ ¼ï¼›æ¯å€‹è¢«é¸å½±æ ¼å†åœ¨ token å…§åšä¸€æ¬¡ FPS é¸ Ktã€‚
   â€” FPS/k-center æœ‰ç¶“å…¸ 2-approx ç†è«–ä¿è­‰ï¼Œå¯é¿é–‹ä»»ä½•ä½ç§©/æ ¸è¿‘ä¼¼ï¼›è®ŠåŒ–é»å‰‡å¯åƒè€ƒ Bayesian online changepoint çš„ç²¾ç¥ï¼ˆæˆ‘å€‘ç”¨ç„¡åƒã€å¯å¾®çš„ç°¡åŒ–åˆ†æ•¸ï¼‰ã€‚([cs.columbia.edu][1]) ([arXiv][2])

**Drop-in å¯¦ä½œï¼ˆPyTorchï¼‰** â€” `FPSChangePointSelector`

> ä»‹é¢èˆ‡å›å‚³æ¬„ä½èˆ‡ä½ ç¾æœ‰ `FrameTokenCoSelector` å°é½Šï¼š`(z, frame_idx, token_idx, frame_mask, token_mask)`ã€‚

```python
import torch, torch.nn as nn
from typing import Optional, Tuple

def _fps_indices_feats(X: torch.Tensor, k: int, seed_idx: int) -> torch.Tensor:
    # X:[T,D]ï¼›greedy FPSï¼Œä¸ç”¨ä»»ä½•ä½ç§©/æ ¸æŠ€å·§
    T = X.size(0); k = min(k, T)
    selected = [int(seed_idx)]
    min_dist = torch.full((T,), float('inf'), device=X.device)
    for _ in range(k - 1):
        last = X[selected[-1]]           # [D]
        dist = torch.norm(X - last[None, :], dim=1)  # [T]
        min_dist = torch.minimum(min_dist, dist)
        min_dist[torch.tensor(selected, device=X.device)] = -1
        selected.append(int(torch.argmax(min_dist).item()))
    return torch.tensor(selected, device=X.device, dtype=torch.long)

def _fps_indices_tokens(F: torch.Tensor, k: int) -> torch.Tensor:
    # F:[N,D]ï¼Œå…ˆæ‰¾é›¢ frame-mean æœ€é çš„ä¸€å€‹ï¼Œå†åš FPS æ“´å¼µ
    N = F.size(0); k = min(k, N)
    mu = F.mean(dim=0, keepdim=True)
    first = int(torch.argmax(torch.norm(F - mu, dim=1)).item())
    selected = [first]
    min_dist = torch.full((N,), float('inf'), device=F.device)
    for _ in range(k - 1):
        last = F[selected[-1]]
        dist = torch.norm(F - last[None, :], dim=1)
        min_dist = torch.minimum(min_dist, dist)
        min_dist[selected] = -1
        selected.append(int(torch.argmax(min_dist).item()))
    return torch.tensor(selected, device=F.device, dtype=torch.long)

class FPSChangePointSelector(nn.Module):
    """
    ä»¥ã€Œè®ŠåŒ–é» seed + FPS è¦†è“‹ã€å–ä»£åŸæœ¬çš„ MLP+ST Top-k å…±é¸å™¨
    è¼¸å…¥ x:[B,T,N,D]ï¼›è¼¸å‡ºèˆ‡åŸ selector å®Œå…¨ä¸€è‡´
    """
    def __init__(self, d_model:int, frame_topk:int, token_topk:int,
                 use_cls:bool=False, ema_alpha:float=0.9):
        super().__init__()
        self.frame_topk, self.token_topk = frame_topk, token_topk
        self.use_cls, self.ema_alpha = use_cls, ema_alpha

    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor]=None
               ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        B,T,N,D = x.shape
        device = x.device
        if mask is None:
            mask = torch.ones(B, T, N, device=device, dtype=x.dtype)

        # frame è¡¨å¾µ [B,T,D]ï¼ˆèˆ‡ä½ ç¾æœ‰å¯¦ä½œç­‰åƒ¹çš„ mean-pool è·¯å¾‘ï¼‰
        if self.use_cls and N >= 1:
            frame_repr = x[:, :, 0, :]
        else:
            denom = mask.sum(dim=2, keepdim=True).clamp_min(1e-6)
            frame_repr = (x * mask.unsqueeze(-1)).sum(dim=2) / denom  # [B,T,D]

        frame_idx = []
        for b in range(B):
            fr = frame_repr[b]  # [T,D]
            # æ–°å¥‡åº¦ seedï¼šèˆ‡ EMA å·®ç•°æœ€å¤§çš„ t
            ema = fr[0].clone()
            novelty = torch.zeros(T, device=device)
            for t in range(T):
                novelty[t] = torch.norm(fr[t] - ema, p=2)
                ema = self.ema_alpha * ema + (1 - self.ema_alpha) * fr[t]
            seed = int(torch.argmax(novelty).item())
            idx_b = _fps_indices_feats(fr, self.frame_topk, seed)  # [Kf]
            frame_idx.append(idx_b)
        frame_idx = torch.stack(frame_idx, dim=0)  # [B,Kf]

        # token FPSï¼ˆé€å½±æ ¼ï¼‰
        token_idx = torch.zeros(B, self.frame_topk, self.token_topk, dtype=torch.long, device=device)
        for b in range(B):
            for i in range(self.frame_topk):
                f = int(frame_idx[b, i].item())
                F = x[b, f]                                   # [N,D]
                valid = mask[b, f] > 0                        # [N]
                F = F[valid]
                idx_tokens = _fps_indices_tokens(F, self.token_topk)
                # å°æ‡‰å›åŸ N çš„ç´¢å¼•
                token_idx[b, i, :len(idx_tokens)] = torch.nonzero(valid, as_tuple=False).squeeze(1)[idx_tokens]

        # gather æˆ z:[B,Kf,Kt,D]ï¼ˆèˆ‡ä½ æª”æ¡ˆå…§æµç¨‹å°é½Šï¼‰
        b_ar = torch.arange(B, device=device)[:, None]
        x_sel_frames = x[b_ar, frame_idx]                        # [B,Kf,N,D]
        b_ar2 = torch.arange(B, device=device)[:, None, None]
        fr_ar2 = torch.arange(self.frame_topk, device=device)[None, :, None]
        z = x_sel_frames[b_ar2, fr_ar2, token_idx]               # [B,Kf,Kt,D]

        # å»º one-hot maskï¼ˆèˆ‡åŸ selector å›å‚³æ¬„ä½ä¸€è‡´ï¼‰
        frame_mask = torch.zeros(B, T, device=device, dtype=x.dtype)
        frame_mask[b_ar, frame_idx] = 1.0
        token_mask = torch.zeros(B, T, N, device=device, dtype=x.dtype)
        token_mask[b_ar2, frame_idx[:, :, None], token_idx] = 1.0

        return z, frame_idx, token_idx, frame_mask, token_mask
```

**ç‚ºä»€éº¼å¯è¡Œ**ï¼šFPS å° k-center çš„ greedy 2-approx æˆç«‹ã€ä¸”è¨ˆç®—é‡æ˜¯ (O(TK_f + NK_t))ï¼ˆæ¯æ¬¡åªå°ã€Œæœ€æ–°é¸é»ã€ç®—è·é›¢ï¼‰ï¼Œå¯¦ä½œç°¡å–®ã€å®Œå…¨ä¸éœ€è¦ä»»ä½•æ ¸/ä½ç§©è¿‘ä¼¼æˆ–åœ–çµæ§‹ï¼›è®ŠåŒ–é» seed è®“å®ƒæ›´åå¥½é—œéµæ™‚åˆ»ã€‚([cs.columbia.edu][1]) ([arXiv][2])

---

# B) å–ä»£ Mem bankï¼ˆå…©å€‹é¸é …ï¼Œçš†éåœ–ã€é SSMï¼‰

## B1) **LatentCrossAttnMemBank**ï¼ˆPerceiver/Set Transformer æ€æƒ³ï¼‰

ä»¥**å°‘é‡å¯å­¸ latent slots**å°æ‰€æœ‰æ™‚åº token åš**äº¤å‰æ³¨æ„åŠ›**å½™æ•´ï¼Œå†æŠŠå½™æ•´è³‡è¨Šå›å¯«åˆ°æ¯å€‹ tokenï¼ˆtokensâ†’latentsâ†’tokensï¼‰ã€‚è¤‡é›œåº¦ (O((TM)\cdot L)) èˆ‡åºåˆ—é•·åº¦ç·šæ€§ï¼Œå¸¸æ•¸å°ã€å·¥ç¨‹å‹å¥½ã€‚éˆæ„Ÿä¾†è‡ª **Perceiver / Perceiver IO** èˆ‡ **Set Transformer (PMA/ISAB)** çš„èª˜å°é»è¨­è¨ˆã€‚([arXiv][3])
ï¼ˆVLM ä¸­ä¹Ÿå¸¸è¦‹ã€ŒPerceiver Resamplerã€æŠŠå¤§ç‰‡è¦–è¦ºç‰¹å¾µå£“é€²å°‘é‡ latentï¼Œå†å›å¯«â€”æ¦‚å¿µç›¸åŒã€‚([arXiv][4])ï¼‰

```python
class LatentCrossAttnMemBank(nn.Module):
    """
    z:[B,T,M,D] -> h:[B,T,M,D]ï¼›ä¸ä½¿ç”¨åœ–/SSM/ä½ç§©æ ¸
    """
    def __init__(self, d_model:int, num_latents:int=64, num_heads:int=8, ff_mult:int=4, dropout:float=0.0):
        super().__init__()
        self.latents = nn.Parameter(torch.randn(num_latents, d_model))
        self.enc_attn = nn.MultiheadAttention(d_model, num_heads, batch_first=True, dropout=dropout)
        self.dec_attn = nn.MultiheadAttention(d_model, num_heads, batch_first=True, dropout=dropout)
        self.ffn = nn.Sequential(
            nn.LayerNorm(d_model),
            nn.Linear(d_model, ff_mult*d_model), nn.GELU(), nn.Linear(ff_mult*d_model, d_model),
        )
        self.norm_in = nn.LayerNorm(d_model)
        self.norm_out = nn.LayerNorm(d_model)

    def forward(self, z: torch.Tensor, valid_mask: Optional[torch.Tensor]=None):
        B,T,M,D = z.shape
        x = z.reshape(B, T*M, D)                                # [B,S,D], S=TM
        x = self.norm_in(x)
        q = self.latents.unsqueeze(0).expand(B, -1, -1)         # [B,L,D]

        # encode: latents attend to tokens
        lat, _ = self.enc_attn(q, x, x, key_padding_mask=None)  # [B,L,D]

        # decode: tokens attend back to latents
        y, _ = self.dec_attn(x, lat, lat)                       # [B,S,D]
        y = self.ffn(y) + y
        h = self.norm_out(x + y).reshape(B, T, M, D)
        return h, {"latents": lat.detach()}
```

## B2) **TemporalConvMemBank**ï¼ˆTCN é¢¨æ ¼ï¼Œåƒ… 1D å·ç©ï¼‰

è‹¥ä½ åå¥½**å®Œå…¨ä¸ä½¿ç”¨æ³¨æ„åŠ›**ï¼Œå°±ç”¨**æ·±åº¦å¯åˆ†é›¢ 1D conv + è†¨è„¹ç‡**ï¼ˆå¯é›™å‘ã€éå› æœï¼‰åœ¨æ™‚é–“è»¸ä¸Šèšåˆï¼›é€™åœ¨åºåˆ—ä»»å‹™ä¸Šå·²è¢«ç³»çµ±æ€§æ¯”è¼ƒéï¼Œå¸¸èƒ½ä»¥æ¥µå°å¸¸æ•¸é …æ‹¿åˆ°å¾ˆå¥½ trade-offã€‚([arXiv][5])

```python
class TemporalConvMemBank(nn.Module):
    """
    z:[B,T,M,D] -> h:[B,T,M,D]ï¼›TCN é¢¨æ ¼ï¼Œä¸ç”¨åœ–/SSM/ä½ç§©æ ¸
    """
    def __init__(self, d_model:int, kernel_size:int=5, dilations=(1,2,4), dropout:float=0.0):
        super().__init__()
        layers = []
        for d in dilations:
            pad = d * (kernel_size - 1) // 2
            layers += [
                nn.Conv1d(d_model, d_model, kernel_size, padding=pad, dilation=d, groups=d_model),  # depthwise
                nn.Conv1d(d_model, d_model, 1),  # pointwise
                nn.GELU(),
                nn.Dropout(dropout),
            ]
        self.layers = nn.ModuleList(layers)
        self.norm = nn.LayerNorm(d_model)

    def forward(self, z: torch.Tensor, valid_mask: Optional[torch.Tensor]=None):
        B,T,M,D = z.shape
        x = z.permute(0,2,3,1).reshape(B*M, D, T)  # [B*M,D,T]
        y = x
        for i in range(0, len(self.layers), 4):
            dw = self.layers[i](y)
            pw = self.layers[i+1](dw)
            act = self.layers[i+2](pw)
            y = self.layers[i+3](act) + y          # æ®˜å·®
        h = y.reshape(B, M, D, T).permute(0, 3, 1, 2)  # [B,T,M,D]
        h = self.norm(h)
        return h, {}
```

> è‹¥è¦**å±€éƒ¨è¦–çª—æ³¨æ„åŠ›**ç‰ˆæœ¬ï¼ˆéä½ç§©è¿‘ä¼¼ï¼‰ï¼šå¯ä»¥æŠŠ `z` å±•æˆ `[B*M,T,D]`ï¼Œå°æ¯æ¢åºåˆ—åš**æ»‘å‹•è¦–çª—è‡ªæ³¨æ„åŠ›**ï¼ˆLongformer çš„ sliding window æƒ³æ³•åœ¨ 1D æ™‚é–“è»¸ä¸Šå¾ˆè‡ªç„¶ï¼‰ã€‚é€™åŒæ¨£ä¸æ¶‰åœ–/SSM/ä½ç§©æ ¸ã€‚([arXiv][6])

---

## ä¸²æ¥æ–¹å¼ï¼ˆå°é½Šä½ ç¾æœ‰æª”æ¡ˆï¼‰

* ä½ çš„ pipeline å½¢ç‹€æ…£ä¾‹èˆ‡ forward I/O å¦‚ä¸‹ï¼š`x:[B,T,N,D] â†’ selector â†’ z:[B,Kf,Kt,D]`ï¼›`z:[B,T,M,D] â†’ mem_bank â†’ h:[B,T,M,D]`ã€‚ä¸Šé¢ä¸‰å€‹é¡åˆ¥å·²**å®Œå…¨éµå®ˆ**ï¼Œèˆ‡åŸ `gather`/å›å‚³æ¬„ä½èªç¾©ä¸€è‡´ï¼Œå¯ç›´æ¥æ›¿æ›æª”æ¡ˆä¸­çš„ `FrameTokenCoSelector` èˆ‡ `GraphBasedMemBank`ã€‚  

---

## å°çµ & å»ºè­°çš„æœ€å°æ›¿æ›é †åº

1. **å…ˆæ› Mem bank**ï¼š`GraphBasedMemBank â†’ LatentCrossAttnMemBank`ï¼ˆæˆ– `TemporalConvMemBank`ï¼‰ï¼Œèƒ½ç«‹åˆ»å»æ‰å»ºåœ–/kNN/GRU è¨˜æ†¶ï¼Œå¸¸æ•¸é …æ˜é¡¯ä¸‹é™ã€‚
2. **å†æ›é¸å–å™¨**ï¼š`FrameTokenCoSelector â†’ FPSChangePointSelector`ï¼Œä¸å†ä¾è³´ MLP æ‰“åˆ†èˆ‡ ST çš„è¨“ç·´æŠ€å·§ï¼Œæ¨è«–æœŸå°¤å…¶å¿«ã€‚
3. **é©—è­‰**ï¼šå›ºå®š ViTï¼Œé‡æ¸¬ `(clips/s, GPU mem, Top-1)`ï¼›FPS çš„æŒ‘é¸å° Kf/Kt çš„å¯æ“´å±•æ€§å¥½ï¼Œæ™‚é–“/ç©ºé–“éƒ½ç·šæ€§ã€‚

---

### åƒè€ƒï¼ˆæ¦‚å¿µä¾æ“šï¼‰

* **k-center 2-approx / Farthest-Firstï¼ˆFPSï¼‰**ï¼šGonzalez, 1985ï¼›ç°¡å–® greedy å³å¾— 2-approxã€‚([cs.columbia.edu][1])
* **ç·šä¸Šè®ŠåŒ–é»**ï¼šAdams & MacKay, 2007ï¼ˆæœ¬æ–‡æ¡å…¶ç²¾ç¥ï¼Œå¯¦ä½œç‚ºç„¡åƒæ–°å¥‡åº¦åˆ†æ•¸ï¼‰ã€‚([arXiv][2])
* **Set Transformerï¼ˆPMA/ISABï¼‰** èˆ‡ **Perceiver/Perceiver-IOï¼ˆlatent cross-attentionï¼‰**ï¼šç”¨å°é‡èª˜å°/latent å¸æ”¶å¤§é›†åˆï¼Œå†å›å¯«åˆ°æ¯å€‹å…ƒç´ ã€‚([Proceedings of Machine Learning Research][7])
* **Longformer**ï¼ˆæ»‘å‹•è¦–çª—æ³¨æ„åŠ›ï¼Œå¯åš 1D æ™‚é–“ç‰ˆï¼‰ï¼š([arXiv][6])
* **TCN**ï¼ˆæ™‚åº 1D å·ç©åœ¨åºåˆ—ä»»å‹™ä¸Šçš„ç³»çµ±æ€§è¡¨ç¾ï¼‰ï¼š([arXiv][5])

[1]: https://www.cs.columbia.edu/~verma/classes/uml/ref/clustering_minimize_intercluster_distance_gonzalez.pdf "CLUSTERING TO MINIMIZE THE MAXIMUM ..."
[2]: https://arxiv.org/abs/0710.3742 "Bayesian Online Changepoint Detection"
[3]: https://arxiv.org/abs/2103.03206 "Perceiver: General Perception with Iterative Attention"
[4]: https://arxiv.org/pdf/2204.14198 "ğŸ¦© Flamingo: a Visual Language Model for Few-Shot Learning"
[5]: https://arxiv.org/abs/1803.01271 "An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling"
[6]: https://arxiv.org/abs/2004.05150 "[2004.05150] Longformer: The Long-Document Transformer"
[7]: https://proceedings.mlr.press/v97/lee19d/lee19d.pdf "A Framework for Attention-based Permutation-Invariant Neural ..."
